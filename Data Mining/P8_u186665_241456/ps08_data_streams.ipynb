{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Rubén Vera Martínez</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">ruben.vera01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">29/11/2022</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = \"movie_lines.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "# Producer in Python that reads a filename by words\n",
    "def read_by_words(filename, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, \"rt\", encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Regular expression to identify words having 3 letters or more and beginning with a-z\n",
    "        word_expr = re.compile('^[a-z]{2,}$', re.IGNORECASE)\n",
    "\n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split(\"\\t\")\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for word in nltk.word_tokenize(text):\n",
    "                          \n",
    "                if word_expr.match(word):\n",
    "                    counter += 1\n",
    "                    \n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print(\"- Read %d words so far\" % (counter))\n",
    "                        else:\n",
    "                            print(\"- Read %d/%d words so far\" % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current word 'of'\n",
      "Current word 'you'\n",
      "Current word 'do'\n",
      "Current word 'and'\n",
      "Current word 'did'\n",
      "Current word 'and'\n",
      "Current word 'man'\n",
      "Current word 'me'\n",
      "Current word 'me'\n",
      "- Read 100000/300000 words so far\n",
      "Current word 'you'\n",
      "Current word 'who'\n",
      "Current word 'something'\n",
      "Current word 'ca'\n",
      "Current word 'your'\n",
      "Current word 'like'\n",
      "- Read 200000/300000 words so far\n",
      "Current word 'got'\n",
      "Current word 'shit'\n",
      "Current word 'on'\n",
      "Current word 'databases'\n",
      "Current word 'big'\n",
      "Current word 'sort'\n",
      "Current word 'me'\n",
      "- Read 300000/300000 words so far\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "# Iterate through the file\n",
    "for word in read_by_words(INPUT_FILE, max_words=300000, report_every=100000):\n",
    "    # Prints 1/10000 of words\n",
    "    if random.random() < 0.0001:\n",
    "        print(\"Current word '%s'\" % (word)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    #If reservoir is not full, we just add the item\n",
    "    if(len(reservoir) < reservoir_size):    \n",
    "        reservoir.append(item)\n",
    "    #If it's full, we discard randomly one item and put there the new item.\n",
    "    else:\n",
    "        reservoir[random.randint(0,max_reservoir_size-1)] = item\n",
    "\n",
    "    assert(len(reservoir) <= max_reservoir_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(filename, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "\n",
    "    words_read = 0\n",
    "    \n",
    "    for word in read_by_words(filename, max_words=max_words, report_every=report_every):\n",
    "        words_read+=1\n",
    "        #If it's not full, call the function\n",
    "        if(len(reservoir) < reservoir_size):\n",
    "            add_to_reservoir(reservoir, word, reservoir_size)\n",
    "        else:\n",
    "        #If it's full, with probability 1-s/n we ignore the item\n",
    "            rand_value = random.random()\n",
    "            if rand_value < 1-reservoir_size/words_read:\n",
    "                continue\n",
    "            #with probability s/n we add the item discarding another item.\n",
    "            else:\n",
    "                add_to_reservoir(reservoir, word, reservoir_size)\n",
    "        \n",
    "    return (words_read, reservoir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 100000/1000000 words so far\n",
      "- Read 200000/1000000 words so far\n",
      "- Read 300000/1000000 words so far\n",
      "- Read 400000/1000000 words so far\n",
      "- Read 500000/1000000 words so far\n",
      "- Read 600000/1000000 words so far\n",
      "- Read 700000/1000000 words so far\n",
      "- Read 800000/1000000 words so far\n",
      "- Read 900000/1000000 words so far\n",
      "- Read 1000000/1000000 words so far\n",
      "Number of items seen    : 1000028\n",
      "Number of items sampled : 1000\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1000\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=1000000, report_every=100000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 you\n",
      "35 to\n",
      "25 the\n",
      "19 it\n",
      "16 and\n",
      "15 of\n",
      "14 that\n",
      "14 just\n",
      "13 with\n",
      "13 do\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:10]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9 % you\n",
      "3.5000000000000004 % to\n",
      "2.5 % the\n",
      "1.9 % it\n",
      "1.6 % and\n",
      "1.5 % of\n",
      "1.4000000000000001 % that\n",
      "1.4000000000000001 % just\n",
      "1.3 % with\n",
      "1.3 % do\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:10]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    #Relative frequency = absolute/size-->in percentage: *100\n",
    "    print((absolute_frequency/len(reservoir)*100), \"%\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 100000/1000000 words so far\n",
      "- Read 200000/1000000 words so far\n",
      "- Read 300000/1000000 words so far\n",
      "- Read 400000/1000000 words so far\n",
      "- Read 500000/1000000 words so far\n",
      "- Read 600000/1000000 words so far\n",
      "- Read 700000/1000000 words so far\n",
      "- Read 800000/1000000 words so far\n",
      "- Read 900000/1000000 words so far\n",
      "- Read 1000000/1000000 words so far\n",
      "Reservoir size: 3000\n",
      " \n",
      "Number of items seen    : 1000028\n",
      "Number of items sampled : 100\n",
      "Absolute freq: 4, Relative freq: 0.048780, word: you\n",
      "Absolute freq: 3, Relative freq: 0.036585, word: we\n",
      "Absolute freq: 3, Relative freq: 0.036585, word: that\n",
      "Absolute freq: 3, Relative freq: 0.036585, word: of\n",
      "Absolute freq: 2, Relative freq: 0.024390, word: what\n",
      "- Read 100000/1000000 words so far\n",
      "- Read 200000/1000000 words so far\n",
      "- Read 300000/1000000 words so far\n",
      "- Read 400000/1000000 words so far\n",
      "- Read 500000/1000000 words so far\n",
      "- Read 600000/1000000 words so far\n",
      "- Read 700000/1000000 words so far\n",
      "- Read 800000/1000000 words so far\n",
      "- Read 900000/1000000 words so far\n",
      "- Read 1000000/1000000 words so far\n",
      "Reservoir size: 100\n",
      " \n",
      "Number of items seen    : 1000028\n",
      "Number of items sampled : 3000\n",
      "Absolute freq: 151, Relative freq: 0.151000, word: you\n",
      "Absolute freq: 109, Relative freq: 0.109000, word: the\n",
      "Absolute freq: 80, Relative freq: 0.080000, word: to\n",
      "Absolute freq: 68, Relative freq: 0.068000, word: it\n",
      "Absolute freq: 62, Relative freq: 0.062000, word: do\n"
     ]
    }
   ],
   "source": [
    "#We do the reservoir sampling with size 100 and 3000 and print the absolute and relative frequency of top 5 words of each.\n",
    "reservoir_size = 100\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=1000000, report_every=100000)\n",
    "print(\"Reservoir size: 3000\\n \")\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "    \n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"Absolute freq: %d, Relative freq: %f, word: %s\" % (absolute_frequency, absolute_frequency/len(freq), word))\n",
    "    \n",
    "\n",
    "    \n",
    "reservoir_size = 3000\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=1000000, report_every=100000)\n",
    "print(\"Reservoir size: 100\\n \")\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "    \n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"Absolute freq: %d, Relative freq: %f, word: %s\" % (absolute_frequency, absolute_frequency/len(freq), word))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir size:  1000 \n",
      " \n",
      "Number of items seen    : 2944884\n",
      "Number of items sampled : 1000\n",
      "Absolute freq: 58, Relative freq: 0.123932, word: you\n",
      "Absolute freq: 28, Relative freq: 0.059829, word: to\n",
      "Absolute freq: 26, Relative freq: 0.055556, word: the\n",
      "Reservoir size:  1500 \n",
      " \n",
      "Number of items seen    : 2944884\n",
      "Number of items sampled : 1500\n",
      "Absolute freq: 69, Relative freq: 0.112195, word: you\n",
      "Absolute freq: 61, Relative freq: 0.099187, word: the\n",
      "Absolute freq: 34, Relative freq: 0.055285, word: do\n",
      "Reservoir size:  5000 \n",
      " \n",
      "Number of items seen    : 2944884\n",
      "Number of items sampled : 5000\n",
      "Absolute freq: 232, Relative freq: 0.164190, word: you\n",
      "Absolute freq: 157, Relative freq: 0.111111, word: the\n",
      "Absolute freq: 136, Relative freq: 0.096249, word: to\n"
     ]
    }
   ],
   "source": [
    "#We do the same as before but with reservoir_size = 1000, 1500 and 3000.\n",
    "reservoir_size = 1000\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=-1, report_every=-1)\n",
    "print(\"Reservoir size: \", reservoir_size, \"\\n \")\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "    \n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:3]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"Absolute freq: %d, Relative freq: %f, word: %s\" % (absolute_frequency, absolute_frequency/len(freq), word))\n",
    "    \n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=-1, report_every=-1)\n",
    "print(\"Reservoir size: \", reservoir_size, \"\\n \")\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "    \n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:3]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"Absolute freq: %d, Relative freq: %f, word: %s\" % (absolute_frequency, absolute_frequency/len(freq), word))\n",
    "\n",
    "\n",
    "reservoir_size = 5000\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, reservoir_size, max_words=-1, report_every=-1)\n",
    "print(\"Reservoir size: \", reservoir_size, \"\\n \")\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "    \n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:3]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"Absolute freq: %d, Relative freq: %f, word: %s\" % (absolute_frequency, absolute_frequency/len(freq), word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above output, with reservoir_size = 1000 we obtain different top3 words than for reservoir_size = 1500 but we obtain the same than for reservoir_size = 5000, so that would be the minimum size of reservoir. In the case of 1000, I founded some execution that founded the same 3 words, but it was not as consistent as it was on 1500 reservoir size. So, for this reasons I'd use reservoir_size = 1500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate on pass 1: 131072 distinct words\n",
      "Estimate on pass 2: 65536 distinct words\n",
      "Estimate on pass 3: 32768 distinct words\n",
      "Estimate on pass 4: 16384 distinct words\n",
      "Estimate on pass 5: 32768 distinct words\n",
      "Estimate on pass 6: 131072 distinct words\n",
      "Estimate on pass 7: 16384 distinct words\n",
      "Estimate on pass 8: 8192 distinct words\n",
      "Estimate on pass 9: 65536 distinct words\n",
      "Estimate on pass 10: 16384 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 10\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    R = []\n",
    "    for word in read_by_words(INPUT_FILE, max_words=1000000, report_every=-1):\n",
    "        #For each word we compute a hush value of the random hash function and then append to an array, the number of trailing zeroes\n",
    "        hash_value = hash_function(word)\n",
    "        R.append(count_trailing_zeroes(hash_value))\n",
    "    #Then, do 2^R to estimate the number of distinct words\n",
    "    estimate = pow(2,(max(R)))\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 51609.6\n",
      "* Median  of estimates: 32768.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF PASSES = 10:\n",
      "\n",
      "Estimate on pass 1: 16 distinct words\n",
      "Estimate on pass 2: 13 distinct words\n",
      "Estimate on pass 3: 14 distinct words\n",
      "Estimate on pass 4: 22 distinct words\n",
      "Estimate on pass 5: 13 distinct words\n",
      "Estimate on pass 6: 19 distinct words\n",
      "Estimate on pass 7: 18 distinct words\n",
      "Estimate on pass 8: 15 distinct words\n",
      "Estimate on pass 9: 12 distinct words\n",
      "Estimate on pass 10: 13 distinct words\n",
      "* Average of estimates: 15.5\n",
      "* Median  of estimates: 14.5\n",
      "Estimate on pass 1: 12 distinct words\n",
      "Estimate on pass 2: 18 distinct words\n",
      "Estimate on pass 3: 13 distinct words\n",
      "Estimate on pass 4: 15 distinct words\n",
      "Estimate on pass 5: 13 distinct words\n",
      "Estimate on pass 6: 12 distinct words\n",
      "Estimate on pass 7: 16 distinct words\n",
      "Estimate on pass 8: 19 distinct words\n",
      "Estimate on pass 9: 14 distinct words\n",
      "Estimate on pass 10: 12 distinct words\n",
      "* Average of estimates: 14.4\n",
      "* Median  of estimates: 13.5\n",
      "Estimate on pass 1: 13 distinct words\n",
      "Estimate on pass 2: 13 distinct words\n",
      "Estimate on pass 3: 15 distinct words\n",
      "Estimate on pass 4: 19 distinct words\n",
      "Estimate on pass 5: 19 distinct words\n",
      "Estimate on pass 6: 17 distinct words\n",
      "Estimate on pass 7: 15 distinct words\n",
      "Estimate on pass 8: 16 distinct words\n",
      "Estimate on pass 9: 18 distinct words\n",
      "Estimate on pass 10: 13 distinct words\n",
      "* Average of estimates: 15.8\n",
      "* Median  of estimates: 15.5\n",
      "NUMBER OF PASSES = 20:\n",
      "\n",
      "Estimate on pass 1: 19 distinct words\n",
      "Estimate on pass 2: 19 distinct words\n",
      "Estimate on pass 3: 23 distinct words\n",
      "Estimate on pass 4: 12 distinct words\n",
      "Estimate on pass 5: 19 distinct words\n",
      "Estimate on pass 6: 12 distinct words\n",
      "Estimate on pass 7: 15 distinct words\n",
      "Estimate on pass 8: 16 distinct words\n",
      "Estimate on pass 9: 15 distinct words\n",
      "Estimate on pass 10: 18 distinct words\n",
      "Estimate on pass 11: 12 distinct words\n",
      "Estimate on pass 12: 12 distinct words\n",
      "Estimate on pass 13: 12 distinct words\n",
      "Estimate on pass 14: 13 distinct words\n",
      "Estimate on pass 15: 18 distinct words\n",
      "Estimate on pass 16: 13 distinct words\n",
      "Estimate on pass 17: 13 distinct words\n",
      "Estimate on pass 18: 19 distinct words\n",
      "Estimate on pass 19: 18 distinct words\n",
      "Estimate on pass 20: 12 distinct words\n",
      "* Average of estimates: 15.5\n",
      "* Median  of estimates: 15.0\n",
      "Estimate on pass 1: 12 distinct words\n",
      "Estimate on pass 2: 16 distinct words\n",
      "Estimate on pass 3: 13 distinct words\n",
      "Estimate on pass 4: 15 distinct words\n",
      "Estimate on pass 5: 13 distinct words\n",
      "Estimate on pass 6: 15 distinct words\n",
      "Estimate on pass 7: 12 distinct words\n",
      "Estimate on pass 8: 18 distinct words\n",
      "Estimate on pass 9: 13 distinct words\n",
      "Estimate on pass 10: 15 distinct words\n",
      "Estimate on pass 11: 13 distinct words\n",
      "Estimate on pass 12: 13 distinct words\n",
      "Estimate on pass 13: 13 distinct words\n",
      "Estimate on pass 14: 12 distinct words\n",
      "Estimate on pass 15: 13 distinct words\n",
      "Estimate on pass 16: 12 distinct words\n",
      "Estimate on pass 17: 18 distinct words\n",
      "Estimate on pass 18: 12 distinct words\n",
      "Estimate on pass 19: 16 distinct words\n",
      "Estimate on pass 20: 12 distinct words\n",
      "* Average of estimates: 13.8\n",
      "* Median  of estimates: 13.0\n",
      "Estimate on pass 1: 13 distinct words\n",
      "Estimate on pass 2: 18 distinct words\n",
      "Estimate on pass 3: 13 distinct words\n",
      "Estimate on pass 4: 19 distinct words\n",
      "Estimate on pass 5: 15 distinct words\n",
      "Estimate on pass 6: 12 distinct words\n",
      "Estimate on pass 7: 19 distinct words\n",
      "Estimate on pass 8: 17 distinct words\n",
      "Estimate on pass 9: 13 distinct words\n",
      "Estimate on pass 10: 18 distinct words\n",
      "Estimate on pass 11: 14 distinct words\n",
      "Estimate on pass 12: 15 distinct words\n",
      "Estimate on pass 13: 18 distinct words\n",
      "Estimate on pass 14: 19 distinct words\n",
      "Estimate on pass 15: 13 distinct words\n",
      "Estimate on pass 16: 12 distinct words\n",
      "Estimate on pass 17: 16 distinct words\n",
      "Estimate on pass 18: 13 distinct words\n",
      "Estimate on pass 19: 16 distinct words\n",
      "Estimate on pass 20: 12 distinct words\n",
      "* Average of estimates: 15.2\n",
      "* Median  of estimates: 15.0\n",
      "UNLIMITED MAX WORDS:\n",
      "\n",
      "Estimate on pass 1: 13 distinct words\n",
      "Estimate on pass 2: 13 distinct words\n",
      "Estimate on pass 3: 13 distinct words\n",
      "Estimate on pass 4: 19 distinct words\n",
      "Estimate on pass 5: 12 distinct words\n",
      "Estimate on pass 6: 13 distinct words\n",
      "Estimate on pass 7: 13 distinct words\n",
      "Estimate on pass 8: 13 distinct words\n",
      "Estimate on pass 9: 12 distinct words\n",
      "Estimate on pass 10: 18 distinct words\n",
      "* Average of estimates: 13.9\n",
      "* Median  of estimates: 13.0\n"
     ]
    }
   ],
   "source": [
    "print(\"NUMBER OF PASSES = 10:\\n\")\n",
    "number_of_passes = 10\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    R = []\n",
    "    for word in read_by_words(INPUT_FILE, max_words=1000000, report_every=-1):\n",
    "        hash_value = hash_function(word)\n",
    "        R.append(count_trailing_zeroes(hash_value))\n",
    "    estimate = 2^(max(R))\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))\n",
    "\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    R = []\n",
    "    for word in read_by_words(INPUT_FILE, max_words=1000000, report_every=-1):\n",
    "        hash_value = hash_function(word)\n",
    "        R.append(count_trailing_zeroes(hash_value))\n",
    "    estimate = 2^(max(R))\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))\n",
    "\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    R = []\n",
    "    for word in read_by_words(INPUT_FILE, max_words=1000000, report_every=-1):\n",
    "        hash_value = hash_function(word)\n",
    "        R.append(count_trailing_zeroes(hash_value))\n",
    "    estimate = 2^(max(R))\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))\n",
    "\n",
    "print(\"NUMBER OF PASSES = 20:\\n\")\n",
    "number_of_passes = 20\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    R = []\n",
    "    for word in read_by_words(INPUT_FILE, max_words=1000000, report_every=-1):\n",
    "        hash_value = hash_function(word)\n",
    "        R.append(count_trailing_zeroes(hash_value))\n",
    "    estimate = 2^(max(R))\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))\n",
    "\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    R = []\n",
    "    for word in read_by_words(INPUT_FILE, max_words=1000000, report_every=-1):\n",
    "        hash_value = hash_function(word)\n",
    "        R.append(count_trailing_zeroes(hash_value))\n",
    "    estimate = 2^(max(R))\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))\n",
    "\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    R = []\n",
    "    for word in read_by_words(INPUT_FILE, max_words=1000000, report_every=-1):\n",
    "        hash_value = hash_function(word)\n",
    "        R.append(count_trailing_zeroes(hash_value))\n",
    "    estimate = 2^(max(R))\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))\n",
    "\n",
    "print(\"UNLIMITED MAX WORDS:\\n\")\n",
    "\n",
    "number_of_passes = 10\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    hash_function = random_hash_function()\n",
    "    R = []\n",
    "    for word in read_by_words(INPUT_FILE, max_words=-1, report_every=-1):\n",
    "        hash_value = hash_function(word)\n",
    "        R.append(count_trailing_zeroes(hash_value))\n",
    "    estimate = 2^(max(R))\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n",
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"code","source":["!pip install -q pgmpy\n","!gdown -q --id 1b2ryMxJOhAOhiPmphsS-0sJkJKuLJ9D4\n","!gdown -q --id 19xqIcNf2WvcG3Hi95d4zv7CpMaJPriMe\n","!gdown -q --id 1pD496PHFtTydlXqnY4s8JpFanP46bO0E\n","!gdown -q --id 1kOSQm_PBSXR1rEr7-6xStUyfja3vXH3l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpEFtEuK3LXQ","executionInfo":{"status":"ok","timestamp":1678380385121,"user_tz":-60,"elapsed":16660,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"bbd6a440-c179-4f87-df52-1adb8a248d07"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h/usr/local/lib/python3.9/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","metadata":{"id":"aWEPYhPy3Jxh"},"source":["# Inference in Hidden Markov Models\n","\n","\n","In this lab session you will work with hidden Markov models (HMMs) using the [pgmpy library](http://www.pgmpy.org). By the end of the session you will be able to\n","\n","- Understand how to learn **hidden Markov models** from data\n","- Implement the **belief propagation** to answer **filtering and prediction** queries\n","- **Implement the Viterbi algorithm** for finding the most likely sequence of hidden states, given some evidence\n","- Experiment with two tasks: a robot navigating on a grid, and how to improve a mispelled text\n","\n","This practice is inspired by https://www.cs.princeton.edu/courses/archive/fall12/cos402/assignments/programs/viterbi/"]},{"cell_type":"markdown","metadata":{"id":"RDSujyY-3Jxm"},"source":["## The Hidden Markov Model\n","\n","A HMM is defined by a Markov chain over hidden variables $h_{1:T}=h_1,h_2,...,h_T$ and it is defined by\n","- a probability distribution over the initial hidden state $p(h_1)$.\n","- a state transition distribution $p(h_t|h_{t-1})$.\n","\n","Each hidden variable $h_t$ influences a corresponding visible variable $v_t$ through the observation model $p(v_t|h_t)$. The joint distribution can be written as\n","$$p(v_{1:T},h_{1:T}) = p(h_1)p(v_1|h_1)\\prod_{t=2}^T p(h_{t}|h_{t-1})p(v_t|h_t).$$\n","\n","Let's define a class that encodes an HMM. This class will create the HMM with a predefined length ``n_vars``:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VfJ988Gu3Jxn","executionInfo":{"status":"ok","timestamp":1678380396314,"user_tz":-60,"elapsed":6940,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}}},"outputs":[],"source":["from pgmpy.models import FactorGraph\n","\n","class HMM:\n","    def __init__(self, n_vars, prior_fn, transition_fn, observation_fn, h_states, v_states, h_name='h', v_name='v'):\n","        self.h = [f\"{h_name}{i}\" for i in range(n_vars)]\n","        self.v = [f\"{v_name}{i}\" for i in range(n_vars)]\n","        self.variables = self.h + self.v\n","        self.state_names = dict([(h, h_states) for h in self.h] +\n","                                [(v, v_states) for v in self.v])\n","        self.f = self.create_factors(n_vars, prior_fn, transition_fn, observation_fn)\n","        \n","    def create_factors(self, n_vars, prior_fn, transition_fn, observation_fn):\n","        \"\"\"\n","        Given the amount of variables (n_vars) in the hidden markov model, it creates factors for\n","        the prior of h_0, for all the transitions from h_{t-1} to h_t (for t in n_vars), and for\n","        the observation function from h_t to v_t.\n","        Returns a dict where keys are (tuples of) variables and values are factors.\n","        E.g. {\"h_0\": DiscreteFactor,\n","              (\"h_1\", \"h_2\"): DiscreteFactor,\n","              (\"h_1\", \"v_1\"): DiscreteFactor,\n","                           ...\n","              }\n","        \"\"\"\n","        \n","        factors = dict()      \n","        for i in range(n_vars):\n","            if i == 0:\n","                # Prior factor\n","                factors[self.h[i]] = prior_fn(self.h[i])\n","            else:\n","                # Transition factor\n","                factors[(self.h[i-1], self.h[i])] = transition_fn(self.h[i-1], self.h[i])\n","            # Observation factor\n","            factors[(self.h[i], self.v[i])] = observation_fn(self.h[i], self.v[i])\n","        return factors\n","    \n","    def to_factor_graph(self):\n","        G = FactorGraph()\n","        assert set(self.variables) == set(v for f in self.f.values() for v in f.variables)\n","        G.add_nodes_from(self.variables)\n","        G.add_factors(*self.f.values())\n","        G.add_edges_from([(v, f) for f in self.f.values() for v in f.variables])\n","        assert G.check_model()\n","        return G"]},{"cell_type":"markdown","metadata":{"id":"lLy8NpJ83Jxq"},"source":["## Basic setting\n","\n","Before tackling larger problems, and to be able to test our implementations, we will first consider a small version of our previous hidden Markov model of weather change over three days, where we can observe if an umbrella has been used or not. We define this factor graph next:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mO6IuzSc3Jxr","executionInfo":{"status":"ok","timestamp":1678380401403,"user_tz":-60,"elapsed":247,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}}},"outputs":[],"source":["from pgmpy.factors.discrete import DiscreteFactor\n","from pgmpy.models import FactorGraph\n","\n","weather_states = [\"sunny\", \"cloudy\", \"rainy\"]\n","umbrella_states = [True, False]\n","\n","def day_prior(d):\n","    return DiscreteFactor(variables=[d],\n","                          cardinality=[3],\n","                          values=[0.3, 0.4, 0.3],\n","                          state_names={d: weather_states})\n","\n","def day_transition(d1, d2): # P(d2|d1)\n","     return DiscreteFactor(variables=[d1, d2],\n","                           cardinality=[3, 3],\n","                           values=[0.7, 0.25, 0.05,\n","                                   0.25, 0.35, 0.4,\n","                                   0.25, 0.5, 0.25],\n","                           state_names={d1: weather_states,\n","                                        d2: weather_states})\n","\n","def take_umbrella_transition(d, u): # P(u|d)\n","    return DiscreteFactor(variables=[d, u],\n","                          cardinality=[3, 2],\n","                          values=[0.2, 0.8,\n","                                  0.6, 0.4,\n","                                  0.95, 0.05],\n","                          state_names={d: weather_states,\n","                                       u: umbrella_states})\n","\n","# Expand the model for 3 days\n","hmm_weather_3 = HMM(n_vars=3,\n","                    prior_fn=day_prior,\n","                    transition_fn=day_transition,\n","                    observation_fn=take_umbrella_transition,\n","                    h_states=weather_states,\n","                    v_states=umbrella_states,\n","                    h_name=\"w\",\n","                    v_name=\"u\")"]},{"cell_type":"markdown","metadata":{"id":"XQxgRQD-3Jxr"},"source":["## Probabilistic Queries\n","\n","We will implement three types of queries:\n","\n","1. **Filtering** : the probability of the current hidden sate given a partial sequence of observations $p(h_t|v_{1:t})$.\n","2. **Prediction** : the probability of a future hidden sate given a partial sequence of observations $p(h_s|v_{1:t})$, for a $s>t$.\n","3. **Evidence** : the probability of a given a full sequence of observations $p(v_{1:T})$.\n","4. **MAP state** : the most likely hidden trajectory given a full sequence of observations $p(v_{1:T})$.\n","\n","A naive way of computing the MAP of the hidden variables can be:\n","1. Clamp the corresponding visible variables $v_{1:t}$.\n","2. Run BP.\n","3. Compute the joint probability of the corresponding hidden variables.\n","3. Get the assignment with maximum probability.\n","\n","\n","The following code calculates the joint hidden trajectory for the weather example with $T=3$ using belief propagation. Clearly, this approach will not scale to larger models. However, we will use it to check that our code is correct.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"a8ak2V8a3Jxt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678380411819,"user_tz":-60,"elapsed":2206,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"683e45c4-a7bb-4b47-eaf3-2fe8e05bdec4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum probability: 0.1628970250705177\n","State assignment: [('w0', 'rainy'), ('w1', 'cloudy'), ('w2', 'rainy')]\n"]}],"source":["from pgmpy.inference import BeliefPropagation\n","import numpy as np\n","\n","bp = BeliefPropagation(hmm_weather_3.to_factor_graph())\n","# Compute the joint probability\n","joint = bp.query(variables=['w0','w1','w2'],\n","                 evidence={\"u0\":True,\"u1\":True, \"u2\":True})\n","# Get the index of the maximum value\n","amax = np.argmax(joint.values)\n","print(\"Maximum probability:\", joint.values.flatten()[amax])\n","print(\"State assignment:\", sorted(joint.assignment([amax])[0])) # pgmpy's assignment function gives us the states for the given index"]},{"cell_type":"markdown","metadata":{"id":"-Kv52Egu3Jxu"},"source":["## Task 1: Robot Navigation on a Grid\n","\n","In this problem, a robot is wandering through the following small world:\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1Rq-izqt8vkkkX6FbWrhaeCuGkgggJiPF\"  alt=\"A robot on a maze\" title=\"Title text\" width=350 height=350>\n","\n","The robot can only occupy the colored squares.  At each time step, the robot attempts to move up, down, left or right, where the choice of direction is made at random.  If the robot attempts to move onto a black square, or to leave the confines of its world, its action has no effect and it does not move at all.  The robot can only sense the color of the square it occupies.  However, its sensors are only $90\\%$ accurate, meaning that $10\\%$ of the time, it perceives a random color rather than the true color of the currently occupied square.  The robot begins each walk in a randomly chosen colored square.\n","\n","In this problem, state refers to the location of the robot in the world in x:y coordinates, and output refers to a perceived color (r, g, b or y).  Thus, a typical random walk looks like this:\n","\n","``3:3 r``\n","``3:3 r``\n","``3:4 y``\n","``2:4 b``\n","``3:4 y``\n","``3:3 r``\n","``2:3 b``\n","``1:3 g``\n","``2:3 b``\n","``2:4 r``\n","``3:4 y``\n","``4:4 y``\n","\n","Here, the robot begins in square ``3:3`` perceiving red, attempts to make an illegal move (to the right), so stays in ``3:3``, still perceiving red.  On the next step, the robot moves up to ``3:4`` perceiving yellow, then left to ``2:4`` perceiving blue (erroneously), and so on.\n","\n","We will provide the HMM model for this world. Then, given only sensor information (i.e., a sequence of colors), you will have to answer different queries regarding the actual path taken by the robot through its world.\n","\n","The data for this problem is in [robot.data](files/robot.data), a file containing $200$ training sequences (random walks) and $200$ test sequences, each sequence consisting of $200$ steps.\n","\n","We provide next the implementation on how to build the HMM. First, we define some functions that will become handy:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"y3wnnbc13Jxv","executionInfo":{"status":"ok","timestamp":1678380420573,"user_tz":-60,"elapsed":235,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}}},"outputs":[],"source":["from pgmpy.factors.discrete import TabularCPD\n","\n","def split(s, sep):\n","    \"\"\"\n","    Generator that splits a sequence s into subsequences, when separator sep is found.\n","    \"\"\"\n","    chunk = []\n","    for val in s:\n","        if val == sep:\n","            yield chunk\n","            chunk = []\n","        else:\n","            chunk.append(val)\n","    yield chunk\n","\n","def to_cpd(phi, x):\n","    \"\"\"\n","    Returns a TabularCPD object from a DiscreteFactor. For a given factor phi(x_0, ..., x_n)\n","    and a variable x_i, it interprets the factor as a CPD P(x_i|Y), where Y is the set of all\n","    variables in phi except x_i. I.e. P(x_i|x_0, ..., x_{i-1}, x_{i+1}, ..., x_n).\n","    It also checks that the factor is a valid conditional probability distribution.\n","    \"\"\"\n","    assert x in phi.variables\n","    idx = phi.variables.index(x)\n","    card = list(phi.cardinality)\n","    var_card = card[idx]\n","    evidence_card = card[:idx] + card[idx+1:]\n","    values = np.moveaxis(phi.values, idx, 0) # move variable x to dimension 0\n","    return TabularCPD(variable=x,\n","                      variable_card=var_card,\n","                      evidence=phi.variables[:idx] + phi.variables[idx+1:],\n","                      evidence_card=evidence_card,\n","                      values=values.reshape(var_card, int(np.prod(evidence_card))), # int cast since np.prod([])=1.0\n","                      state_names=phi.state_names)\n","\n","def normalize_cpd_values(variables, cardinality, values):\n","    \"\"\"\n","    Normalize a numpy array of CPD values. It also accounts for unreachable states (all 0s).\n","    \"\"\"\n","    assert len(variables) in (1,2)\n","    f = DiscreteFactor(variables=variables, cardinality=cardinality, values=values)\n","    # Normalize it as a CPD\n","    f = to_cpd(f, variables[-1]) # Get the last variable for the CPD (e.g. [ht-1, ht] -> p(ht|ht-1))\n","    f.normalize()\n","    # Remove NaNs, put 0s instead\n","    f.values[np.logical_not(np.isfinite(f.values))] = 0.0\n","    if len(f.variables) == 2:\n","        # The process f->cpd swapped the variables, let's swap them back\n","        assert f.variables[1] == variables[0]\n","        return np.transpose(f.values)\n","    return f.values\n","    \n","def get_hmm_factors(trajectories, h_states, v_states):\n","    \"\"\"\n","    Gets the prior, transition and observation probabilities of an HMM from data.\n","    \"\"\"\n","    prior = np.zeros(len(h_states))\n","    transition = np.zeros([len(h_states), len(h_states)])\n","    observation = np.zeros([len(h_states), len(v_states)])\n","    for t in trajectories:\n","        for i in range(len(t)):\n","            s, o = t[i].split(\" \") # h and v come as a string \"h v\"\n","            s_i, o_i = h_states.index(s), v_states.index(o)\n","            if i == 0: # Prior\n","                prior[s_i] += 1\n","            else:\n","                prev_s_i = h_states.index(t[i-1].split(\" \")[0])\n","                transition[prev_s_i][s_i] += 1 # Transition\n","            observation[s_i][o_i] += 1 # Observation\n","\n","    return normalize_cpd_values(['h0'], [len(h_states)], prior), \\\n","           normalize_cpd_values(['ht-1', 'ht'], [len(h_states), len(h_states)], transition), \\\n","           normalize_cpd_values(['ht', 'vt'], [len(h_states), len(v_states)], observation)"]},{"cell_type":"markdown","metadata":{"id":"daFL8Gwu3Jxw"},"source":["Then, we extract the probabilities from the training sequences, and print one sample trajectory:"]},{"cell_type":"code","execution_count":6,"metadata":{"scrolled":true,"id":"zE-MW19f3Jxw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678380425105,"user_tz":-60,"elapsed":220,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"6c9d261d-4f9f-43d3-feb5-b1f9903b6dc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["ROBOT: 401 robot trajectories read.\n","['2:4', '3:4', '3:4', '4:4', '4:4', '4:4', '4:4', '4:4', '4:4', '3:4']\n","['r', 'y', 'y', 'b', 'b', 'r', 'b', 'b', 'b', 'y']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/pgmpy/factors/discrete/CPD.py:332: RuntimeWarning: invalid value encountered in true_divide\n","  tabular_cpd.values = (cpd / cpd.sum(axis=0)).reshape(tabular_cpd.cardinality)\n"]}],"source":["with open(\"robot.data\") as f:\n","    robot_data = f.read().splitlines() # this accounts for '\\n'\n","\n","# Split into train and test data (separated by \"..\" in the file, end of single trajectory is marked with \".\")\n","train_robot, test_robot = [list(split(dataset, '.')) for dataset in split(robot_data, '..')]\n","print(f\"ROBOT: {len(train_robot) + len(test_robot)} robot trajectories read.\")\n","\n","size_square = 4\n","positions = [f\"{x+1}:{y+1}\" for x in range(size_square) for y in range(size_square)]\n","colors = ['r', 'g', 'b', 'y']\n","r_prior, r_transition, r_observation = get_hmm_factors(trajectories = train_robot,\n","                                                       h_states=positions,\n","                                                       v_states=colors) # This will raise a warning: invalid value encountered in true_divide\n","\n","# Define factor functions\n","def robot_prior(pos):\n","    return DiscreteFactor(variables=[pos],\n","                          cardinality=[len(positions)],\n","                          values=r_prior,\n","                          state_names = {pos: positions})\n","\n","def robot_transition(prev_pos, next_pos):\n","    return DiscreteFactor(variables=[prev_pos, next_pos],\n","                          cardinality=[len(positions), len(positions)],\n","                          values=r_transition,\n","                          state_names = {prev_pos: positions,\n","                                         next_pos: positions})\n","\n","def robot_observation(pos, col):\n","    return DiscreteFactor(variables=[pos, col],\n","                          cardinality=[len(positions), len(colors)],\n","                          values=r_observation,\n","                          state_names = {pos: positions,\n","                                         col: colors})\n","\n","robot_pos_trajectories = [[f\"{p.split(' ')[0]}\" for p in traj] for traj in test_robot][:-1] # last one is empty\n","robot_color_trajectories = [[f\"{p.split(' ')[1]}\" for p in traj] for traj in test_robot][:-1]\n","\n","print(robot_pos_trajectories[-1][:10])\n","print(robot_color_trajectories[-1][:10])"]},{"cell_type":"markdown","metadata":{"id":"5IdeX5Lt3Jxx"},"source":["### Questions\n","\n","Given the following sequence of observations $v_{1:T}$:\n","```\n","['g', 'g', 'b', 'r', 'r', 'b', 'b', 'r', 'r', 'y', 'r', 'b', 'r', 'y', 'b']\n","```\n","\n","Answer the following questions using your implementation of belief propagation:\n","1. Filtering : what is $p(h_6|v_{1:6})$?\n","2. Prediction : what is $p(h_7|v_{1:6})$?\n","3. Probability of evidence : what is $p(v_{1:T})$?"]},{"cell_type":"code","source":["from traitlets.traitlets import ValidateHandler\n","from functools import reduce\n","import operator\n","from collections import defaultdict\n","from copy import deepcopy\n","\n","def prod(iterable):\n","    \n","    \"\"\"\n","      Returns the product of all the items in the iterable given in as input.\n","    \"\"\"\n","\n","    return reduce(operator.mul, iterable, 1)\n","\n","class MyBeliefPropagation:\n","    def __init__(self, factor_graph):\n","        assert factor_graph.check_model()\n","        self.original_graph = factor_graph\n","        self.variables = factor_graph.get_variable_nodes()\n","        \n","        self.state_names = dict()\n","        for f in self.original_graph.factors:\n","            self.state_names.update(f.state_names)\n","        self.bp_done = False\n","\n","    def factor_ones(self, v):\n","        \"\"\"\n","        Returns a DiscreteFactor for variable v with all ones.\n","        \"\"\"\n","        return DiscreteFactor(variables=[v],\n","                        cardinality=[len(self.state_names.get(v))],\n","                        values=[1]*len(self.state_names.get(v)),\n","                        state_names=self.state_names)\n","        \n","    def initialize_messages(self):\n","        \"\"\"\n","        This function creates, for each edge factor-variable, two messages: m(f->v) and \n","        m(v->f). It initiliazies each message as a DiscreteFactor with all ones. It stores all\n","        the messages in a dict of dict. Keys of both dicts are either factors or variables.\n","        Messages are indexed as messages[to][from]. For example, m(x->y) is in messages[y][x].\n","        It's done this way because it will be useful to get all messages that go to a variable\n","        or a factor.\n","        \"\"\"\n","        self.messages = defaultdict(dict)\n","        for f in self.working_graph.get_factors():\n","            for v in f.variables:\n","              self.messages[f][v] = self.factor_ones(v) # to f from v\n","              self.messages[v][f] = self.factor_ones(v) # to v from f\n","                \n","    def factor_to_variable(self, f, v):\n","        \"\"\"\n","        Computes message m from factor to variable. It computes it from all messages from all\n","        other variables to the factor (i.e. all variables connected the factor except v).\n","        Returns message m.\n","        \"\"\"\n","        assert v in self.variables and f in self.working_graph.factors\n","        M = list(self.messages[f].values())     # we take all the messages to f\n","        M.remove(self.messages[f][v])           # we remove the message from v to f\n","        res = f * prod(M)                       # apply the formula from theory (we multiply the factor to the product of messages to f, except from the variable v)\n","        vs = []\n","        for aux in res.variables:\n","          if aux!= v:\n","            vs.append(aux)                      # in vs, we store all the variables connected to the factor except v\n","        return res.marginalize(vs, inplace=False)     # finally, we marginalize (sum over all values of vs)\n"," \n","    def variable_to_factor(self, v, f):\n","        \"\"\"\n","        Computes message m from variable to factor. It computes it from all messages from all\n","        other factors to the variable (i.e. all factors connected the variable except f).\n","        Returns message m.\n","        \"\"\"        \n","        assert v in self.variables and f in self.working_graph.factors\n","        M = list(self.messages[v].values())     # we take all the messages to v\n","        M.remove(self.messages[v][f])           # we remove the message from f to v\n","        return prod(M)\n","    \n","    def get_evidence_factors(self, evidence):\n","        \"\"\"\n","        For each evidence variable v, create a factor with p(v=e)=1. Recieves a dict of\n","        evidence, where keys are variables and values are variable states. Returns a list of\n","        DiscreteFactor.\n","        \"\"\"\n","        df = []\n","        for var, state in evidence.items():\n","          vals = []\n","          for st in self.state_names.get(var):\n","            if (st == state):\n","              vals.append(1)\n","            else:\n","              vals.append(0)\n","          factor = DiscreteFactor(variables=[var],        # we create a factor that contains 1 only in the value observed, otherwise 0\n","                        cardinality=[len(self.state_names.get(var))],\n","                        values = vals,\n","                        state_names=self.state_names)\n","          df.append(factor)\n","        return df\n","        \n","    def update(self, m_to, m_from):\n","        \"\"\"\n","        Performs an update of a message depending on whether it is variable-to-factor or\n","        factor-to-variable.\n","        \"\"\"\n","        if (m_to in self.variables):\n","          self.messages[m_to][m_from] = self.factor_to_variable(m_from, m_to)     # we update the message factor-to-variable if the destinatary of the message is a variable\n","        elif (m_to in self.working_graph.get_factors()):\n","          self.messages[m_to][m_from] = self.variable_to_factor(m_from, m_to)     # we update the message variable-to-factor if the destinatary of the message is a factor\n","    \n","    def collect_evidence(self, node, parent=None):         # apply the algorithm of collect_evidence seen in theory\n","        \"\"\"\n","        Passes messages from the leaves to the root of the tree.\n","        The parent argument is used to avoid an infinite recursion.\n","        \"\"\"\n","        for child in self.messages[node]:\n","          if child != parent:       # we do this to avoid infinite recursion\n","            self.update(node, self.collect_evidence(child, parent=node))      \n","        return node\n","    \n","    def distribute_evidence(self, node, parent=None):       # apply the algorithm of distribute_evidence seen in theory\n","        \"\"\"\n","        Passes messages from the root to the leaves of the tree.\n","        The parent argument is used to avoid an infinite recursion.\n","        \"\"\"\n","        for child in self.working_graph.neighbors(node):\n","          if child != parent:         # we do this to avoid infinite recursion\n","            self.update(child, node)\n","            self.distribute_evidence(child, parent=node)                  \n","    \n","    def set_evidence(self, evidence):\n","        \"\"\"\n","        Generates a new graph with the evidence factors\n","        evidence (keys: variables, values: states)\n","        \"\"\"\n","        evidence_factors = self.get_evidence_factors(evidence)\n","        self.working_graph = self.original_graph.copy()\n","        for f in evidence_factors:\n","            self.working_graph.add_factors(f)           # first, we add the factor that will contain the evidence\n","            for variable in f.variables:\n","              self.working_graph.add_edge(variable, f)      # and then we add an edge between their neighbours\n","        self.bp_done = False\n","    \n","    def run_bp(self, root):\n","        \"\"\"\n","        After initializing the messages, this function performs Belief Propagation\n","        using collect_evidence and distribute_evidence from the given root node.\n","        \"\"\"\n","        assert root in self.variables, \"Variable not in the model\"\n","        # we call the correspondent functions in each case\n","        self.initialize_messages()\n","        self.distribute_evidence(root)\n","        self.collect_evidence(root)\n","\n","        self.bp_done = True\n","\n","    def get_marginal(self, variable):\n","        \"\"\"\n","        To be used after run_bp. Returns p(variable | evidence) unnormalized.\n","        \"\"\"\n","        assert self.bp_done, \"First run BP!\"\n","        marginal = 1\n","        for factors in self.working_graph.neighbors(variable):\n","          marginal *= self.messages[variable][factors]          # the product of the incoming messages from the factors\n","        return marginal       # we return the marginal unnormalized"],"metadata":{"id":"7OtFhhK6QO8I","executionInfo":{"status":"ok","timestamp":1678380435151,"user_tz":-60,"elapsed":4,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YPPmmEiQ3Jxy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678381435043,"user_tz":-60,"elapsed":3032,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"98730d09-ccbd-4918-85b8-6b3a72b2e5ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------FILTERING:--------------------\n","+----------------+------------------+\n","| position5      |   phi(position5) |\n","+================+==================+\n","| position5(1:1) |           0.0000 |\n","+----------------+------------------+\n","| position5(1:2) |           0.0109 |\n","+----------------+------------------+\n","| position5(1:3) |           0.0039 |\n","+----------------+------------------+\n","| position5(1:4) |           0.0000 |\n","+----------------+------------------+\n","| position5(2:1) |           0.0000 |\n","+----------------+------------------+\n","| position5(2:2) |           0.0000 |\n","+----------------+------------------+\n","| position5(2:3) |           0.8905 |\n","+----------------+------------------+\n","| position5(2:4) |           0.0353 |\n","+----------------+------------------+\n","| position5(3:1) |           0.0002 |\n","+----------------+------------------+\n","| position5(3:2) |           0.0081 |\n","+----------------+------------------+\n","| position5(3:3) |           0.0092 |\n","+----------------+------------------+\n","| position5(3:4) |           0.0267 |\n","+----------------+------------------+\n","| position5(4:1) |           0.0006 |\n","+----------------+------------------+\n","| position5(4:2) |           0.0002 |\n","+----------------+------------------+\n","| position5(4:3) |           0.0000 |\n","+----------------+------------------+\n","| position5(4:4) |           0.0143 |\n","+----------------+------------------+\n","\n","--------------------PREDICTION:--------------------\n","+----------------+------------------+\n","| position6      |   phi(position6) |\n","+================+==================+\n","| position6(1:1) |           0.0000 |\n","+----------------+------------------+\n","| position6(1:2) |           0.0842 |\n","+----------------+------------------+\n","| position6(1:3) |           0.0821 |\n","+----------------+------------------+\n","| position6(1:4) |           0.0000 |\n","+----------------+------------------+\n","| position6(2:1) |           0.0819 |\n","+----------------+------------------+\n","| position6(2:2) |           0.0000 |\n","+----------------+------------------+\n","| position6(2:3) |           0.0842 |\n","+----------------+------------------+\n","| position6(2:4) |           0.0829 |\n","+----------------+------------------+\n","| position6(3:1) |           0.0832 |\n","+----------------+------------------+\n","| position6(3:2) |           0.0836 |\n","+----------------+------------------+\n","| position6(3:3) |           0.0823 |\n","+----------------+------------------+\n","| position6(3:4) |           0.0848 |\n","+----------------+------------------+\n","| position6(4:1) |           0.0833 |\n","+----------------+------------------+\n","| position6(4:2) |           0.0847 |\n","+----------------+------------------+\n","| position6(4:3) |           0.0000 |\n","+----------------+------------------+\n","| position6(4:4) |           0.0829 |\n","+----------------+------------------+\n","\n","--------------------EVIDENCE:--------------------\n","P[v_1:T] 3.023443633501055\n"]}],"source":["observed_colors = ['g', 'g', 'b', 'r', 'r', 'b', 'b', 'r', 'r', 'y', 'r', 'b', 'r', 'y', 'b']\n","\n","robot_HMM = HMM(n_vars=len(observed_colors),\n","                prior_fn=robot_prior,\n","                transition_fn=robot_transition,\n","                observation_fn=robot_observation,\n","                h_states=positions,\n","                v_states=colors,\n","                h_name=\"position\",\n","                v_name=\"color\")\n","\n","p = dict()\n","\n","# filtering = probability of current time-step (n), given v_1,...,v_n\n","print(\"--------------------FILTERING:--------------------\")\n","my_bp = MyBeliefPropagation(robot_HMM.to_factor_graph())\n","my_bp.set_evidence({\"color0\":observed_colors[0], \"color1\":observed_colors[1], \"color2\":observed_colors[2], \"color3\":observed_colors[3], \"color4\":observed_colors[4], \"color5\":observed_colors[5]})\n","my_bp.run_bp(\"position5\")\n","p[\"h_6|v_1:6\"] = my_bp.get_marginal(\"position5\").normalize(inplace=False)\n","print(p[\"h_6|v_1:6\"])\n","\n","# prediction = probability of next time-step (n + 1), given v_1,...,v_n\n","print(\"\\n--------------------PREDICTION:--------------------\")\n","p[\"h_7|v_1:6\"] = my_bp.get_marginal(\"position6\").normalize(inplace=False)\n","print(p[\"h_7|v_1:6\"])\n","\n","# evidence = probability of the visible variables (v)\n","print(\"\\n--------------------EVIDENCE:--------------------\")\n","my_bp2 = MyBeliefPropagation(robot_HMM.to_factor_graph())\n","my_bp2.set_evidence({\"color0\":observed_colors[0], \"color1\":observed_colors[1], \"color2\":observed_colors[2], \"color3\":observed_colors[3], \"color4\":observed_colors[4], \"color5\":observed_colors[5], \"color6\":observed_colors[6], \"color7\":observed_colors[7], \"color8\":observed_colors[8], \"color9\":observed_colors[9], \"color10\":observed_colors[10], \"color11\":observed_colors[11], \"color12\":observed_colors[12], \"color13\":observed_colors[13], \"color14\":observed_colors[14]})\n","my_bp2.run_bp(\"position0\")\n","p[\"h_15|v_1:T\"] = my_bp2.get_marginal(\"position14\")\n","p[\"v_1:T\"] = sum(p[\"h_15|v_1:T\"].values)\n","print(\"P[v_1:T]\", p[\"v_1:T\"])"]},{"cell_type":"markdown","metadata":{"id":"J_149nsY3Jxy"},"source":["#### SOLUTION:\n","Original trajectory:\n","```\n","robot_pos_trajectories[9][:15]\n","['1:3', '1:3', '2:3', '2:4', '2:4', '2:3', '2:3', '2:4', '2:4', '3:4', '3:3', '2:3', '2:4', '3:4', '4:4']\n","['g', 'g', 'b', 'r', 'r', 'b', 'b', 'r', 'r', 'y', 'r', 'b', 'r', 'y', 'b']\n","```"]},{"cell_type":"markdown","metadata":{"id":"CKjFRhMx3Jxz"},"source":["## Task 2: Correcting typos without a dictionary\n","\n","The second domain deals with the problem of correcting typos in text without using a dictionary.  Here, you will be given text containing many typographical errors and the goal is to correct as many typos as possible.\n","\n","In this problem, state refers to the correct letter that should have been typed, and output refers to the actual letter that was typed.  Given a sequence of outputs (i.e., actually typed letters), the problem is to reconstruct the hidden state sequence (i.e., the intended sequence of letters).  Thus, data for this problem looks like this:\n","\n","<code>\n","i i\n","n n \n","t t\n","r r\n","o o\n","d x\n","u u\n","c c\n","t t\n","i i\n","o i\n","n n\n","_ _\n","t t\n","h h\n","e e\n","_ _\n","</code>\n","\n","\n","where the left column is the correct text and the right column contains text with errors.\n","\n","Data for this problem was generated as follows: we started with a text document, in this case, the Unabomber's Manifesto, which was chosen not for political reasons, but as a convenient, on-line, single-author text of about the right length.  For simplicity, all numbers and punctuation were converted to white space and all letters converted to lower case.  The remaining text is a sequence only over the lower case letters and the space character, represented in the data files by an underscore character.  Next, typos were artificially added to the data as follows: with $90\\%$ probability, the correct letter is transcribed, but with $10\\%$ probability, a randomly chosen neighbor (on an ordinary physical keyboard) of the letter is transcribed instead.  Space characters are always transcribed correctly.  In a harder variant of the problem, the rate of errors is increased to $20\\%$.  The first (roughly) $20,000$ characters of the document have been set aside for testing.  The remaining $161,000$ characters are used for training.\n","\n","As an example, the original document begins:\n","\n","<code>introduction the industrial revolution and its consequences have been a disaster for the human race they have greatly increased the life expectancy of those of us who live in advanced countries but they have destabilized society\n","</code>    \n","    \n","With $20\\%$ noise, it looks like this:\n","\n","<code>introductipn the industfial revolhtjon and its consequences bafw newn a diszster rkr the yumab race thdy have grwatky increased the ljte esoectandy od thosr of is who libe in advanced coubfries but they have fewtabipuzee xociwty</code>\n","\n","The error rate (fraction of  characters that are mistyped) is about $16.5\\%$ (less than $20\\%$ because space characters were not corrupted).\n","\n","Data for this part of the assignment is in [typos10.data](files/typos10.data) and [typos20.data](files/typos20.data), representing data generated with a $10\\%$ or $20\\%$ error rate, respectively.\n","\n","Next, we provide the code to get the HMM from the training data."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"LVmFTP5d3Jxz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678383169796,"user_tz":-60,"elapsed":1934,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"9230abf2-d23c-4399-8f21-fa59d1183b8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["TYPOS 10: 30558 words read.\n","TYPOS 20: 30558 words read.\n","Original text:  introduction the industrial revolution and its consequences have been a disaster for the human \n","Typos 10% text: introxuctiin the ibdudtrial revokufuon anf its consequences ysfe been a disaster for the hyman \n","Typos 20% text: introductipn the industfial revolhtjon and its consequences bafw newn a diszster rkr the yumab \n"]}],"source":["# 10% error rate\n","with open(\"typos10.data\") as f:\n","    text_data = f.read().splitlines() # this accounts for '\\n'\n","\n","# Split into train and test data (separated by \"..\" in the file, end of single word is marked with \"_ _\")\n","train_typos10, test_typos10 = [list(split(dataset, '_ _')) for dataset in split(text_data, '..')]\n","print(f\"TYPOS 10: {len(train_typos10) + len(test_typos10)} words read.\")\n","\n","characters = [chr(i+97) for i in range(26)]\n","t10_prior, t10_transition, t10_observation = get_hmm_factors(trajectories = train_typos10,\n","                                                             h_states=characters,\n","                                                             v_states=characters)\n","\n","# 20% error rate\n","with open(\"typos20.data\") as f:\n","    text_data = f.read().splitlines() # this accounts for '\\n'\n","train_typos20, test_typos20 = [list(split(dataset, '_ _')) for dataset in split(text_data, '..')]\n","print(f\"TYPOS 20: {len(train_typos20) + len(test_typos20)} words read.\")\n","t20_prior, t20_transition, t20_observation = get_hmm_factors(trajectories = train_typos20,\n","                                                             h_states=characters,\n","                                                             v_states=characters)\n","\n","# Define factor functions\n","def text10_prior(c):\n","    return DiscreteFactor(variables=[c],\n","                          cardinality=[len(characters)],\n","                          values=t10_prior,\n","                          state_names = {c: characters})\n","\n","def text10_transition(prev_c, next_c):\n","    return DiscreteFactor(variables=[prev_c, next_c],\n","                          cardinality=[len(characters), len(characters)],\n","                          values=t10_transition,\n","                          state_names = {prev_c: characters,\n","                                         next_c: characters})\n","\n","def text10_observation(c, c_typed):\n","    return DiscreteFactor(variables=[c, c_typed],\n","                          cardinality=[len(characters), len(characters)],\n","                          values=t10_observation,\n","                          state_names = {c: characters,\n","                                         c_typed: characters})\n","\n","def text20_prior(c):\n","    return DiscreteFactor(variables=[c],\n","                          cardinality=[len(characters)],\n","                          values=t20_prior,\n","                          state_names = {c: characters})\n","\n","def text20_transition(prev_c, next_c):\n","    return DiscreteFactor(variables=[prev_c, next_c],\n","                          cardinality=[len(characters), len(characters)],\n","                          values=t20_transition,\n","                          state_names = {prev_c: characters,\n","                                         next_c: characters})\n","\n","def text20_observation(c, c_typed):\n","    return DiscreteFactor(variables=[c, c_typed],\n","                          cardinality=[len(characters), len(characters)],\n","                          values=t20_observation,\n","                          state_names = {c: characters,\n","                                         c_typed: characters})\n","\n","# Get test text from dataset\n","original_text = \" \".join([\"\".join([c.split(\" \")[0] for c in word]) for word in test_typos10])\n","typos10_text = \" \".join([\"\".join([c.split(\" \")[1] for c in word]) for word in test_typos10])\n","typos20_text = \" \".join([\"\".join([c.split(\" \")[1] for c in word]) for word in test_typos20])\n","print(\"Original text: \", original_text[:95])\n","print(\"Typos 10% text:\", typos10_text[:95])\n","print(\"Typos 20% text:\", typos20_text[:95])"]},{"cell_type":"markdown","metadata":{"id":"jIGeK_1B3Jx0"},"source":["### Questions\n","\n","Given the following sequence of observations $v_{1:T}$:\n","```\n","['i', 'n', 't', 'r', 'o', 'x', 'u', 'c', 't', 'i', 'i', 'n']\n","```\n","\n","Answer the following questions:\n","1. Filtering : what is $p(h_{11}|v_{1:11})$?\n","2. Prediction : what is $p(h_{12}|v_{1:11})$?\n","3. Probability of evidence : what is $p(v_{1:12})$?\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"x62ue_Kt3Jx0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678383177801,"user_tz":-60,"elapsed":8010,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"21bf5531-1ae5-420b-9735-b392ecd55cb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------FILTERING:--------------------\n","+--------+------------+\n","| c10    |   phi(c10) |\n","+========+============+\n","| c10(a) |     0.0000 |\n","+--------+------------+\n","| c10(b) |     0.0000 |\n","+--------+------------+\n","| c10(c) |     0.0000 |\n","+--------+------------+\n","| c10(d) |     0.0000 |\n","+--------+------------+\n","| c10(e) |     0.0000 |\n","+--------+------------+\n","| c10(f) |     0.0000 |\n","+--------+------------+\n","| c10(g) |     0.0000 |\n","+--------+------------+\n","| c10(h) |     0.0000 |\n","+--------+------------+\n","| c10(i) |     0.1140 |\n","+--------+------------+\n","| c10(j) |     0.0000 |\n","+--------+------------+\n","| c10(k) |     0.0602 |\n","+--------+------------+\n","| c10(l) |     0.0000 |\n","+--------+------------+\n","| c10(m) |     0.0000 |\n","+--------+------------+\n","| c10(n) |     0.0000 |\n","+--------+------------+\n","| c10(o) |     0.8078 |\n","+--------+------------+\n","| c10(p) |     0.0000 |\n","+--------+------------+\n","| c10(q) |     0.0000 |\n","+--------+------------+\n","| c10(r) |     0.0000 |\n","+--------+------------+\n","| c10(s) |     0.0000 |\n","+--------+------------+\n","| c10(t) |     0.0000 |\n","+--------+------------+\n","| c10(u) |     0.0179 |\n","+--------+------------+\n","| c10(v) |     0.0000 |\n","+--------+------------+\n","| c10(w) |     0.0000 |\n","+--------+------------+\n","| c10(x) |     0.0000 |\n","+--------+------------+\n","| c10(y) |     0.0000 |\n","+--------+------------+\n","| c10(z) |     0.0000 |\n","+--------+------------+\n","\n","--------------------PREDICTION:--------------------\n","+--------+------------+\n","| c11    |   phi(c11) |\n","+========+============+\n","| c11(a) |     0.0122 |\n","+--------+------------+\n","| c11(b) |     0.0137 |\n","+--------+------------+\n","| c11(c) |     0.0461 |\n","+--------+------------+\n","| c11(d) |     0.0248 |\n","+--------+------------+\n","| c11(e) |     0.0445 |\n","+--------+------------+\n","| c11(f) |     0.0876 |\n","+--------+------------+\n","| c11(g) |     0.0334 |\n","+--------+------------+\n","| c11(h) |     0.0008 |\n","+--------+------------+\n","| c11(i) |     0.0243 |\n","+--------+------------+\n","| c11(j) |     0.0000 |\n","+--------+------------+\n","| c11(k) |     0.0023 |\n","+--------+------------+\n","| c11(l) |     0.0645 |\n","+--------+------------+\n","| c11(m) |     0.0547 |\n","+--------+------------+\n","| c11(n) |     0.1787 |\n","+--------+------------+\n","| c11(o) |     0.0211 |\n","+--------+------------+\n","| c11(p) |     0.0323 |\n","+--------+------------+\n","| c11(q) |     0.0003 |\n","+--------+------------+\n","| c11(r) |     0.1265 |\n","+--------+------------+\n","| c11(s) |     0.0498 |\n","+--------+------------+\n","| c11(t) |     0.0545 |\n","+--------+------------+\n","| c11(u) |     0.0672 |\n","+--------+------------+\n","| c11(v) |     0.0205 |\n","+--------+------------+\n","| c11(w) |     0.0354 |\n","+--------+------------+\n","| c11(x) |     0.0004 |\n","+--------+------------+\n","| c11(y) |     0.0031 |\n","+--------+------------+\n","| c11(z) |     0.0014 |\n","+--------+------------+\n","\n","--------------------EVIDENCE:--------------------\n","P[v_1:12] 4.907482236670567e-16\n"]}],"source":["word = ['i', 'n', 't', 'r', 'o', 'x', 'u', 'c', 't', 'i', 'i', 'n']\n","\n","word_hmm = HMM(n_vars=len(word),\n","               prior_fn=text10_prior,\n","               transition_fn=text10_transition,\n","               observation_fn=text10_observation,\n","               h_states=characters,\n","               v_states=characters,\n","               h_name=\"c\",\n","               v_name=\"c_typed\")\n","\n","p2 = dict()\n","\n","# filtering = probability of current time-step (n), given v_1,...,v_n\n","print(\"--------------------FILTERING:--------------------\")\n","my_bp = MyBeliefPropagation(word_hmm.to_factor_graph())\n","my_bp.set_evidence({\"c_typed0\":word[0], \"c_typed1\":word[1], \"c_typed2\":word[2], \"c_typed3\":word[3], \"c_typed4\":word[4], \"c_typed5\":word[5], \"c_typed6\":word[6], \"c_typed7\":word[7], \"c_typed8\":word[8], \"c_typed9\":word[9], \"c_typed10\":word[10]})\n","my_bp.run_bp(\"c10\")\n","p2[\"h_11|v_1:11\"] = my_bp.get_marginal(\"c10\").normalize(inplace=False)\n","print(p2[\"h_11|v_1:11\"])\n","\n","# prediction = probability of next time-step (n + 1), given v_1,...,v_n\n","print(\"\\n--------------------PREDICTION:--------------------\")\n","my_bp.run_bp(\"c11\")\n","p2[\"h_12|v_1:11\"] = my_bp.get_marginal(\"c11\").normalize(inplace=False)\n","print(p2[\"h_12|v_1:11\"])\n","\n","# evidence = probability of the visible variables (v)\n","print(\"\\n--------------------EVIDENCE:--------------------\")\n","my_bp2 = MyBeliefPropagation(word_hmm.to_factor_graph())\n","my_bp.set_evidence({\"c_typed0\":word[0], \"c_typed1\":word[1], \"c_typed2\":word[2], \"c_typed3\":word[3], \"c_typed4\":word[4], \"c_typed5\":word[5], \"c_typed6\":word[6], \"c_typed7\":word[7], \"c_typed8\":word[8], \"c_typed9\":word[9], \"c_typed10\":word[10], \"c_typed11\":word[11]})\n","my_bp.run_bp(\"c11\")\n","p2[\"h_12|v_1:12\"] = my_bp.get_marginal(\"c11\")\n","p2[\"v_1:12\"] = sum(p2[\"h_12|v_1:12\"].values)\n","print(\"P[v_1:12]\", p2[\"v_1:12\"])"]},{"cell_type":"markdown","metadata":{"id":"vCHTwbdC3Jx1"},"source":["## The Viterbi algorithm\n","\n","We have seen how we can obtain the MAP query from a BP query, and how that approach did not scale. Let's implement now the Viterbi algorithm to perform MAP queries on a HMM. It consists of the following steps\n","\n","1. Compute the factors $\\mu_t$ that will recursively be used to obtain the maximum probability\n","\n","$$\\mu(h_{t-1}) = \\max_{h_t} p(v_t|h_t)p(h_t|h_{t-1})\\mu(h_t),\\qquad 2\\leq t\\leq T,$$\n","$$\\mu(h_T) = 1.$$\n","\n","2. Obtain the desired maximum probability and backtrack to obtain the state trajectory $h^*_{1:T}$ using the previous computations\n","\n","$$p_\\text{max} = \\text{max}_{h_1} p(v_1|h_1)p(h_1)\\mu(h_1),$$\n","$$h_1^* = \\text{argmax}_{h_1} p(v_1|h_1)p(h_1)\\mu(h_1),$$\n","$$h_t^* = \\text{argmax}_{h_t} p(v_t|h_t)p(h_t|h_{t-1}^*)\\mu(h_t).$$"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"nXaJCWPr3Jx1","executionInfo":{"status":"ok","timestamp":1678383177802,"user_tz":-60,"elapsed":40,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}}},"outputs":[],"source":["def factor_ones(v, state_names):\n","    \"\"\"\n","    Returns a DiscreteFactor with all ones for variable v with its domain defined in states_names.\n","    \"\"\"\n","    card = len(state_names[v])\n","    return DiscreteFactor(variables=[v],\n","                          cardinality=[card],\n","                          values=np.ones(card),\n","                          state_names=state_names)\n","\n","class Viterbi:\n","    def max_and_argmax(self, f):\n","        \"\"\"\n","        Given a factor f, returns its maximum value and the corresponding assignment. We assume that f\n","        is a DiscreteFactor of one variable.\n","        \"\"\"\n","        assert len(f.variables)==1, \"Factor connected to more than one variable: \"+str(f.variables)\n","        v = f.variables[0]\n","        am = np.argmax(f.values)\n","        return f.values[am], list(f.state_names[v])[am]\n","    \n","    def compute_messages(self, hmm, evidence):\n","        \"\"\"\n","        Given an HMM and the evidence (a list of states in order from left to right), compute the\n","        messages (from right to left).\n","        Returns a list of messages.\n","        \"\"\"\n","        n_vars = len(evidence)\n","        messages = [None]*n_vars\n","        messages[n_vars-1] = 1    # the last (left->right) position contains a 1\n","        for i in range(n_vars-1,0,-1):\n","          # We calculate the maximum multiplication value between the probability of v_t given h_t, of h_t given h_(t-1) and the message incoming from the right\n","          # with the reduce we look the evidence of the variable\n","          messages[i-1] = (hmm.f[(hmm.h[i], hmm.v[i])].reduce([(hmm.v[i], evidence[i])], inplace=False)*hmm.f[(hmm.h[i-1], hmm.h[i])]*messages[i]).maximize(variables=[hmm.h[i]], inplace=False)\n","        assert all(m is not None for m in messages)\n","        return messages\n","    \n","    def backtrack(self, hmm, messages, evidence):\n","        \"\"\"\n","        Given an HMM, the messages (computed from right to left), and the evidence (a list of states\n","        in order from left to right), it computes the MAP states as well as their value in the joint\n","        distribution.\n","        Returns a list of states and the joint probability of these states.\n","        \"\"\"\n","        p_v0h0 = hmm.f[(hmm.h[0],hmm.v[0])].reduce([(hmm.v[0], evidence[0])], inplace=False)\n","        value, h0_opt = self.max_and_argmax(p_v0h0*hmm.f[hmm.h[0]]*messages[0])\n","        # we store the first h^*:\n","        map_h = [h0_opt]               \n","        # we use the same formula but now we reduce it with the optimal h that we have calculated\n","        joint = p_v0h0.reduce([(hmm.h[0], h0_opt)], inplace=False)*hmm.f[hmm.h[0]].reduce([(hmm.h[0], h0_opt)], inplace=False)\n","\n","        for i in range(1, len(hmm.h)):\n","            # we take the previous h^*, because we will use it to compute the current optimal h:\n","            h_opt = map_h[i-1]\n","            p_vh = hmm.f[(hmm.h[i], hmm.v[i])].reduce([(hmm.v[i], evidence[i])], inplace=False)\n","            p_hhminusOne = hmm.f[(hmm.h[i-1],hmm.h[i])].reduce([(hmm.h[i-1], h_opt)], inplace=False)\n","            \n","            value, h_opt = self.max_and_argmax(p_vh*p_hhminusOne*messages[i])\n","            # we store all the h^*:\n","            map_h.append(h_opt)         \n","            # we use the same formula but now we reduce it with the optimal h that we have calculated:\n","            joint *= p_vh.reduce([(hmm.h[i], h_opt)], inplace=False)*p_hhminusOne.reduce([(hmm.h[i], h_opt)], inplace=False)\n","\n","        return map_h, value\n","    \n","    def map_query(self, hmm, evidence):\n","        \"\"\"\n","        Given an hmm and the evidence (a list of states in order from left to right), returns the\n","        MAP states as well as the MAP probability.\n","        \"\"\"\n","        assert type(evidence) in (list, tuple), \"The evidence should be a list of observed states\"\n","        assert len(evidence) == len(hmm.v), \"To get the MAP of the states we need the whole sequence of observed states\"\n","        messages = self.compute_messages(hmm, evidence)\n","        return self.backtrack(hmm, messages, evidence)"]},{"cell_type":"markdown","metadata":{"id":"719oapWn3Jx2"},"source":["### MAP queries in the Basic setting\n","First we try it in our simple setting, and check that the result is the same as from the BP query."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"xCxAtCNe3Jx2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678383177803,"user_tz":-60,"elapsed":39,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"52f02406-f45b-42a4-d35c-ba9d23ae3b72"},"outputs":[{"output_type":"stream","name":"stdout","text":["MAP states: ['rainy', 'cloudy', 'rainy']\n","MAP prob: 0.38\n"]}],"source":["viterbi = Viterbi()\n","map_states, map_prob = viterbi.map_query(hmm_weather_3, evidence=[True, True, True])\n","print(\"MAP states:\", map_states)\n","print(\"MAP prob:\", map_prob)"]},{"cell_type":"markdown","metadata":{"id":"VBcsE2JC3Jx3"},"source":["How does this method compare, in terms of complexity, to our previous, naive, approach?"]},{"cell_type":"markdown","metadata":{"id":"HIsNoBeh3Jx3"},"source":["Viterbi Algorithm is more computationally efficient than belief propagation in terms of time complexity, especially for large models and long sequences. In the case of BP we compute the factors of all the model, which is unnecessary. In Viterbi, first we compute max value efficiently and then do backtrack. \n"]},{"cell_type":"markdown","metadata":{"id":"vZ9i7HfW3Jx4"},"source":["### MAP queries in the robot navigation setting\n","\n","To try it in the robot navigation setting, let's first define the DiscreteFactor functions for the HMM model, according to the values extracted from the training data:"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"hZAEqXNA3Jx4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678383177804,"user_tz":-60,"elapsed":33,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"e2a6618d-2a1c-43ed-b080-96f065fd6707"},"outputs":[{"output_type":"stream","name":"stdout","text":["Traj: ['g', 'g', 'r', 'r', 'g', 'r', 'g', 'g', 'r', 'r', 'g', 'r', 'r', 'r', 'r', 'r', 'g', 'r', 'b', 'g', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'y', 'r', 'g', 'r', 'r', 'g', 'g', 'r', 'r', 'r', 'r', 'r', 'g', 'g', 'g', 'b', 'b', 'r', 'b', 'g', 'g', 'g', 'g', 'g', 'y', 'b', 'b', 'y', 'g', 'y', 'y', 'y', 'b', 'y', 'g', 'y', 'g', 'g', 'r', 'b', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g', 'g', 'b', 'r', 'r', 'r', 'b', 'r', 'y', 'y', 'g', 'g', 'b', 'r', 'r', 'b', 'g', 'b', 'g', 'r', 'y', 'r', 'y', 'b', 'g', 'r', 'b', 'r', 'y', 'b', 'y', 'y', 'r', 'g', 'b', 'y', 'y', 'y', 'g', 'b', 'y', 'b', 'b', 'b', 'y', 'g', 'y', 'b', 'y', 'g', 'y', 'y', 'b', 'b', 'y', 'y', 'y', 'b', 'b', 'y', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'y', 'y', 'y', 'r', 'y', 'y', 'g', 'y', 'y', 'r', 'g', 'g', 'b', 'b', 'b', 'b', 'b', 'b', 'y', 'y', 'y', 'g', 'r', 'b', 'g', 'g', 'g', 'g', 'g', 'b', 'r', 'b', 'b', 'b', 'g', 'g', 'b', 'r', 'b', 'r', 'b', 'g', 'y', 'b']\n","Infered traj.:  ['1:3', '1:3', '1:2', '1:2', '1:3', '1:2', '1:3', '1:3', '1:2', '1:2', '1:3', '1:2', '1:2', '1:2', '1:2', '1:2', '1:3', '1:2', '1:2', '1:3', '1:3', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:3', '1:2', '1:2', '1:3', '1:3', '1:2', '1:2', '1:2', '1:2', '1:2', '1:3', '1:3', '1:3', '2:3', '2:3', '2:4', '2:3', '3:3', '3:2', '3:2', '3:2', '3:2', '4:2', '4:1', '4:1', '3:1', '3:2', '4:2', '4:2', '4:2', '4:1', '3:1', '2:1', '3:1', '3:2', '3:2', '3:3', '2:3', '1:3', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:3', '1:3', '2:3', '2:4', '2:4', '2:4', '2:3', '2:4', '2:3', '1:3', '1:3', '1:3', '2:3', '2:4', '2:4', '2:3', '1:3', '2:3', '2:4', '2:4', '3:4', '2:4', '3:4', '3:3', '3:2', '3:3', '2:3', '2:4', '3:4', '4:4', '3:4', '3:4', '3:3', '3:2', '4:2', '4:2', '4:2', '4:2', '3:2', '4:2', '4:2', '4:1', '4:1', '4:1', '3:1', '3:2', '4:2', '4:1', '3:1', '3:2', '4:2', '4:2', '4:1', '4:1', '4:2', '4:2', '4:2', '4:1', '4:1', '4:2', '4:1', '4:1', '4:1', '4:1', '4:1', '4:1', '4:1', '4:2', '4:2', '4:2', '4:2', '4:2', '4:2', '3:2', '3:1', '3:1', '2:1', '2:1', '2:1', '3:1', '4:1', '4:1', '4:1', '4:1', '4:1', '4:2', '4:2', '4:2', '3:2', '3:3', '2:3', '1:3', '1:3', '1:3', '1:3', '1:3', '2:3', '2:4', '2:3', '2:3', '2:3', '1:3', '1:3', '2:3', '2:4', '2:3', '2:4', '2:3', '1:3', '1:3', '2:3']\n","Solution:       ['1:3', '1:3', '1:2', '1:2', '1:3', '1:3', '1:3', '1:3', '1:2', '1:2', '1:3', '1:2', '1:2', '1:2', '1:2', '1:2', '1:3', '1:3', '1:3', '1:3', '1:3', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:3', '1:3', '1:2', '1:2', '1:2', '1:2', '1:2', '1:3', '1:3', '1:3', '2:3', '2:3', '3:3', '3:2', '3:2', '3:2', '3:2', '3:2', '3:2', '4:2', '4:1', '4:1', '3:1', '3:2', '4:2', '4:2', '4:2', '4:1', '3:1', '2:1', '3:1', '3:2', '3:2', '3:3', '2:3', '1:3', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:2', '1:3', '1:3', '2:3', '2:4', '2:4', '2:4', '2:4', '2:4', '2:3', '1:3', '1:3', '1:3', '2:3', '2:4', '2:4', '2:3', '1:3', '2:3', '3:3', '3:3', '3:4', '3:3', '3:4', '3:3', '3:2', '3:3', '2:3', '2:4', '3:4', '4:4', '3:4', '3:4', '3:3', '3:2', '4:2', '4:2', '4:2', '4:2', '4:1', '4:1', '3:1', '4:1', '4:1', '4:1', '3:1', '2:1', '3:1', '4:1', '3:1', '3:2', '3:1', '3:1', '4:1', '4:1', '4:2', '4:2', '4:2', '4:1', '4:1', '3:1', '4:1', '4:1', '4:1', '4:1', '4:1', '4:1', '4:1', '4:2', '4:2', '4:2', '4:1', '4:2', '4:2', '4:2', '4:2', '4:2', '4:2', '3:2', '4:2', '4:1', '4:1', '4:1', '4:1', '4:1', '4:1', '4:2', '4:2', '4:2', '3:2', '3:3', '2:3', '1:3', '1:3', '1:3', '1:3', '1:3', '2:3', '2:4', '2:3', '2:3', '2:3', '1:3', '1:3', '2:3', '2:4', '2:3', '3:3', '2:3', '1:3', '2:3', '2:3']\n","MAP value: 0.2373142165409162\n","Error: 0.145\n"]}],"source":["observed_colors = robot_color_trajectories[4]\n","actual_trajectory = robot_pos_trajectories[4]\n","\n","robot_HMM = HMM(n_vars=len(observed_colors),\n","                prior_fn=robot_prior,\n","                transition_fn=robot_transition,\n","                observation_fn=robot_observation,\n","                h_states=positions,\n","                v_states=colors,\n","                h_name=\"position\",\n","                v_name=\"color\")\n","map_states, map_value = viterbi.map_query(robot_HMM, evidence=observed_colors)\n","\n","print(\"Traj:\", observed_colors)\n","print(\"Infered traj.: \", map_states)\n","print(\"Solution:      \", actual_trajectory)\n","print(\"MAP value:\", map_value)\n","\n","def error(t1, t2):\n","    error = 0\n","    for c1, c2 in zip(t1, t2):\n","        if c1 != c2:\n","            error += 1\n","    return error/len(t1)\n","\n","print(\"Error:\", error(map_states, actual_trajectory))"]},{"cell_type":"markdown","metadata":{"id":"TI7Czszu3Jx4"},"source":["### MAP queries in the typo correction setting\n"]},{"cell_type":"markdown","metadata":{"id":"oDix-T8w3Jx4"},"source":["Correct the following word containing typos with the HMM: ['i','n','t','r','o','x','u','c','t','i','i','n']"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"J4fynBY63Jx4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678383177805,"user_tz":-60,"elapsed":29,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"e22259ab-7735-48b3-fdc1-e89fe691caab"},"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'n', 't', 'r', 'o', 's', 'u', 'c', 't', 'i', 'o', 'n']\n","0.16452253325502034\n"]}],"source":["map_states, map_value = viterbi.map_query(word_hmm, evidence=word)\n","\n","print(map_states)\n","print(map_value)"]},{"cell_type":"markdown","metadata":{"id":"dXEMSps43Jx5"},"source":["Now, let's correct the full text. We have the original text and the one that contains typos in the variables original_text and typos10_text respectively. We provide a function above to test the error between two texts."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"pWyE6Kun3Jx5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678383186534,"user_tz":-60,"elapsed":4678,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"4cf7fafa-91bd-4ffb-a848-d4eabaf706c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Corrected: 0.055173444976076555\n","Not corrected: 0.08248604465709729\n"]}],"source":["rec10_text = []\n","for word in typos10_text.split(\" \"):\n","    chs = [c for c in word]\n","    \n","    # build hmm model\n","    word_hmm_10 = HMM(n_vars=len(word),\n","               prior_fn=text10_prior,\n","               transition_fn=text10_transition,\n","               observation_fn=text10_observation,\n","               h_states=characters,\n","               v_states=characters,\n","               h_name=\"c\",\n","               v_name=\"c_typed\")\n","    \n","    # run viterbi to get MAP states  \n","    new_word = viterbi.map_query(word_hmm_10, evidence = chs)[0]\n","    rec10_text.append(\"\".join(new_word))\n","rec10_text = \" \".join(rec10_text)\n","\n","print(\"Corrected:\", error(rec10_text, original_text))\n","print(\"Not corrected:\", error(typos10_text, original_text))"]},{"cell_type":"markdown","metadata":{"id":"ynytolmY3Jx5"},"source":["Do the same for the case of 20% of error rate."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"dCRJiNCo3Jx5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678383200637,"user_tz":-60,"elapsed":6642,"user":{"displayName":"SARA SORIANO ROSSA","userId":"02602039418296615786"}},"outputId":"89adb179-b774-4c0e-b5de-593dbe8d7f2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Corrected: 0.10925039872408293\n","Not corrected: 0.16143341307814993\n"]}],"source":["rec20_text = []\n","for word in typos20_text.split(\" \"):\n","    chs = [c for c in word]\n","\n","    # build hmm model\n","    word_hmm_20 = HMM(n_vars=len(word),\n","               prior_fn=text20_prior,\n","               transition_fn=text20_transition,\n","               observation_fn=text20_observation,\n","               h_states=characters,\n","               v_states=characters,\n","               h_name=\"c\",\n","               v_name=\"c_typed\")\n","    \n","    # run viterbi to get MAP states\n","    new_word = viterbi.map_query(word_hmm_20, evidence = chs)[0]\n","    rec20_text.append(\"\".join(new_word))\n","rec20_text = \" \".join(rec20_text)\n","\n","print(\"Corrected:\", error(rec20_text, original_text))\n","print(\"Not corrected:\", error(typos20_text, original_text))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[{"file_id":"1mGXR92K34JXtAPfWXUdwbI5WzJeggrXw","timestamp":1677000045243}]}},"nbformat":4,"nbformat_minor":0}